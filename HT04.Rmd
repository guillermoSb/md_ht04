# Hoja de Trabajo 04 - Árboles de Decisión


```{r echo=FALSE}
library(dplyr)
library(knitr)
library(rpart)       # performing regression trees
library(rsample)     # data splitting 
library(rpart.plot)  # plotting regression trees
library(ipred)       # bagging
library(caret)       # bagging
```

## 1 - Separando el dataset en prueba y entrenamiento
```{r echo=FALSE}
data <- read.csv("train.csv")
```

### Limpiando los datos
```{r echo=FALSE}
columns_used <- c()
neighborhoodNames <- c("NoRidge", "NridgHt", "StoneBr", "Timber", "Veenker", "Somerst", "ClearCr", "Crawfor", "CollgCr", "Blmngtn", "Gilbert", "NWAmes", "SawyerW", "Mitchel", "NAmes", "NPkVill", "SWISU", "Blueste", "Sawyer", "OldTown", "Edwards", "BrkSide", "BrDale", "IDOTRR", "MeadowV")

for(n in 1:length(neighborhoodNames)) {
  # Variable minuscula para nuestro uso.
  data$neighborhood[data$Neighborhood == neighborhoodNames[n]] <- n
}
columns_used <- append(columns_used, "neighborhood")

hs <- c("1Story", "2Story",	"1.5Fin",	"SLvl", "SFoyer")

for(n in 1:length(hs)) {
  # Variable minuscula para nuestro uso.
  data$houseStyle[data$HouseStyle == hs[n]] <- n
}
columns_used <- append(columns_used, "houseStyle")

 data$houseZone[data$MSZoning == "A"] <- 1
 data$houseZone[data$MSZoning == "C"] <- 2
 data$houseZone[data$MSZoning == "FV"] <- 3
 data$houseZone[data$MSZoning == "I"] <- 4
 data$houseZone[data$MSZoning == "RH"] <- 5
 data$houseZone[data$MSZoning == "RL"] <- 6
 data$houseZone[data$MSZoning == "RP"] <- 7
 data$houseZone[data$MSZoning == "RM"] <- 8
 columns_used <- append(columns_used, "houseZone")

data$houseUtilities[data$Utilities == "AllPub"] <- 1
data$houseUtilities[data$Utilities == "NoSewr"] <- 2
data$houseUtilities[data$Utilities == "NoSeWa"] <- 3
data$houseUtilities[data$Utilities == "ELO"] <- 4
columns_used <- append(columns_used, "houseUtilities")

data$roadAccess[data$Condition1 == "Artery"] <- 1
data$roadAccess[data$Condition1 == "Feedr"] <- 2
data$roadAccess[data$Condition1 == "Norm"] <- 3
data$roadAccess[data$Condition1 == "RRNn"] <- 4
data$roadAccess[data$Condition1 == "RRAn"] <- 5
data$roadAccess[data$Condition1 == "PosN"] <- 6
data$roadAccess[data$Condition1 == "PosA"] <- 7
data$roadAccess[data$Condition1 == "RRNe"] <- 8
data$roadAccess[data$Condition1 == "RRAe"] <- 9
columns_used <- append(columns_used, "roadAccess")

data$remodelated[data$YearBuilt != data$YearRemodAdd] <- 1
data$remodelated[data$YearBuilt == data$YearRemodAdd] <- 0
columns_used <- append(columns_used, "remodelated")

data$roofStyle[data$RoofStyle == "Flat"]  <- 1
data$roofStyle[data$RoofStyle == "Gable"]  <- 2
data$roofStyle[data$RoofStyle == "Gambrel"]  <- 3
data$roofStyle[data$RoofStyle == "Hip"]  <- 4
data$roofStyle[data$RoofStyle == "Mansard"]  <- 5
data$roofStyle[data$RoofStyle == "Shed"]  <- 6
columns_used <- append(columns_used, "roofStyle")

data$roofMaterial[data$RoofMatl == "ClyTile"] <- 1
data$roofMaterial[data$RoofMatl == "CompShg"] <- 2
data$roofMaterial[data$RoofMatl == "Membran"] <- 3
data$roofMaterial[data$RoofMatl == "Metal"] <- 4
data$roofMaterial[data$RoofMatl == "Roll"] <- 5
data$roofMaterial[data$RoofMatl == "Tar&Grv"] <- 6
data$roofMaterial[data$RoofMatl == "WdShake"] <- 7
data$roofMaterial[data$RoofMatl == "WdShngl"] <- 8
columns_used <- append(columns_used, "roofMaterial")

data$overallQuality <- data$OverallQual
columns_used <- append(columns_used, "overallQuality")

data$overallCondition <- data$OverallCond
columns_used <- append(columns_used, "overallCondition")


data$exteriorCondition[data$ExterCond == "Po"] <- 1
data$exteriorCondition[data$ExterCond == "Fa"] <- 2
data$exteriorCondition[data$ExterCond == "TA"] <- 3
data$exteriorCondition[data$ExterCond == "Gd"] <- 4
data$exteriorCondition[data$ExterCond == "Ex"] <- 5
columns_used <- append(columns_used, "exteriorCondition")

data$foundationMaterial[data$Foundation == "BrkTil"] <- 1
data$foundationMaterial[data$Foundation == "CBlock"] <- 2
data$foundationMaterial[data$Foundation == "PConc"] <- 3
data$foundationMaterial[data$Foundation == "Slab"] <- 4
data$foundationMaterial[data$Foundation == "Stone"] <- 5
data$foundationMaterial[data$Foundation == "Wood"] <- 6
columns_used <- append(columns_used, "foundationMaterial")

data$basement[is.na(data$BsmtQual)] <- 0
data$basement[!is.na(data$BsmtQual)] <- 1
columns_used <- append(columns_used, "basement")

data$basementCondition[data$BsmtCond == "Ex"] <- 3
data$basementCondition[data$BsmtCond == "Gd"] <- 2
data$basementCondition[data$BsmtCond != "Ex"] <- 1
data$basementCondition[data$BsmtCond != "Gd"] <- 1
data$basementCondition[is.na(data$BsmtCond)] <- 0
columns_used <- append(columns_used, "basementCondition")

data$fireplace[is.na(data$FireplaceQu)] <- 0
data$fireplace[!is.na(data$FireplaceQu)] <- 1
columns_used <- append(columns_used, "fireplace")

data$garageArea <- data$GarageArea
columns_used <- append(columns_used, "garageArea")

data$pool[is.na(data$PoolQC)] <- 0
data$pool[!is.na(data$PoolQC)] <- 1
columns_used <- append(columns_used, "pool")

data$additionalFeature[is.na(data$MiscFeature)] <- 0
data$additionalFeature[!is.na(data$MiscFeature)] <- 1
columns_used <- append(columns_used, "additionalFeature")

data$livingArea <- data$GrLivArea
columns_used <- append(columns_used, "livingArea")

data$yearBuilt <- data$YearBuilt
columns_used <- append(columns_used, "yearBuilt")


data$salePrice <- data$SalePrice
columns_used <- append(columns_used, "salePrice")

tv <- c("WD", "Oth", "New", "ConLw", "ConLI", "ConLD", "Con", "CWD", "COD")

for(n in 1:length(tv)) {
  # Variable minuscula para nuestro uso.
  data$saleType[data$SaleType == tv[n]] <- n
}
columns_used <- append(columns_used, "saleType")

msz <- c("FV", "RL", "RH", "RM" , "C (all)")

for(n in 1:length(msz)) {
  # Variable minuscula para nuestro uso.
  data$mSZoning[data$MSZoning == msz[n]] <- n
}
columns_used <- append(columns_used, "mSZoning")

clean_data <- subset(data, select = columns_used)
```

Columnas a utilizar (basándonos en el análisis exploratorio de la hoja anterior):
```{r}
print(paste(columns_used,collapse=' '))
```

Un 75% del dataset se usará para entrenar el árbol.
``` {r}
set.seed(5)
expected_result <- clean_data$salePrice
partition <- createDataPartition(y=expected_result,
                                 p=.75,
                                 list=F)
train_set <- clean_data[partition,]
test_set <- clean_data[-partition,]
```



## 2 - Árbol de regresión

``` {r}
model_1 <- rpart(
  formula = salePrice ~ neighborhood + houseStyle + houseZone + houseUtilities + remodelated + pool + livingArea + yearBuilt
                        + overallQuality + overallCondition,
  data    = train_set,
  method  = "anova"
  )
rpart.plot(model_1)
```
```{r}
nrow(clean_data)
```


## 3 - Realizando predicciones

```{r}
y <- test_set[,c("salePrice")]

y_pred_1 <- predict(model_1, newdata = test_set)
rmse_model_1 <- RMSE(pred = y_pred_1, obs = test_set$salePrice)
abs_error_1 <- abs(mean(y_pred_1 - test_set$salePrice))
mean_error_1 <- y-y_pred_1
mean_error_1 <- mean(mean_error_1)
```

El RMSE para el modelo 1 es `r rmse_model_1` y el error absoluto es `r abs_error_1`. Esto quiere decir que, en promedio hubo un error de USD 3040.

## 4 - Haciendo más modelos
```{r}
model_2 <- rpart(
  formula = salePrice ~ neighborhood + houseStyle + houseZone + houseUtilities + remodelated + pool + livingArea + yearBuilt
                        + overallQuality + overallCondition,
  data    = train_set,
  method  = "anova",
  control = list(maxdepth = 3)
  )
rpart.plot(model_2)
```

```{r}
model_3 <- rpart(
  formula = salePrice ~ neighborhood + houseStyle + houseZone + houseUtilities + remodelated + pool + livingArea + yearBuilt
                        + overallQuality + overallCondition,
  data    = train_set,
  method  = "anova",
  control = list(maxdepth = 2)
  )
rpart.plot(model_3)
```


```{r}
model_4 <- rpart(
  formula = salePrice ~ neighborhood + houseStyle + houseZone + houseUtilities + remodelated + pool + livingArea + yearBuilt
                        + overallQuality + overallCondition,
  data    = train_set,
  method  = "anova",
  control = list(maxdepth = 1)
  )
rpart.plot(model_4)
```

Comparando resultados
```{r echo= FALSE}
# Modelo 2
y_pred_2 <- predict(model_2, newdata = test_set)
rmse_model_2 <- RMSE(pred = y_pred_2, obs = test_set$salePrice)
abs_error_2 <- abs(mean(y_pred_2 - test_set$salePrice))
# Modelo 3
y_pred_3 <- predict(model_3, newdata = test_set)
rmse_model_3 <- RMSE(pred = y_pred_3, obs = test_set$salePrice)
abs_error_3 <- abs(mean(y_pred_3 - test_set$salePrice))
# Modelo 4
y_pred_4 <- predict(model_4, newdata = test_set)
rmse_model_4 <- RMSE(pred = y_pred_4, obs = test_set$salePrice)
abs_error_4 <- abs(mean(y_pred_4 - test_set$salePrice))
```

```{r echo=FALSE}
model_results <- data.frame(
  c("Modelo 1", "Modelo 2", "Modelo 3", "Modelo"),
  c(rmse_model_1, rmse_model_2, rmse_model_3, rmse_model_4),
  c(abs_error_1, abs_error_2, abs_error_3, abs_error_4)
)

colnames(model_results) <- c("","RMSE", "Error Absoluto")
kable(model_results)
```
El modelo 2 tiene un valor menor en el RMSE y en el error absoluto. Podemos utilizar este modelo para compararlo con la regresión lineal utilizada en el ejercicio anterior
## 5 - Comparando con modelo de regresión lineal multivariable
```{r}
multi_variable_model <- lm(salePrice ~ neighborhood + remodelated + roofStyle + overallQuality + overallCondition + garageArea + livingArea + yearBuilt, data=train_set)

prediction_multi<- predict(multi_variable_model, test_set, type="response")
rmse_model_multi <- RMSE(pred = prediction_multi, obs = test_set$salePrice)
abs_error_multi <- abs(mean(prediction_multi - test_set$salePrice))
```

El error absoluto para este modelo es `r abs_error_multi` y el RMSE es `r rmse_model_multi`

## 6 - Variable respuesta
### Análisis exploratorio
```{r}
hist(clean_data$salePrice)
summary(clean_data$salePrice)
```
En el histograma se puede ver un sesgo hacia la izquierda.

```{r}
test_set$economy <- ifelse(test_set$salePrice < 163000, "Economic", ifelse(test_set$salePrice >= 163000 & test_set$salePrice <= 214000, "Average", "Expensive"))

train_set$economy <- ifelse(train_set$salePrice < 163000, "Economic", ifelse(train_set$salePrice >= 163000 & train_set$salePrice <= 214000, "Average", "Expensive"))
```
Dado que en el histograma basado en los precios de casas hay un sesgo hacia la izquierda se toma como medida de tendencia central la mediana ya que esta nos permite saber la ubicación central de los datos. Además, se toma en cuenta que los cuartiles nos dan una referencia de donde poder colocar los rangos de precios.


## 7 - Árbol de clasificación utilizando la variable respuesta
```{r}
train_set <- train_set %>% mutate_at(c("economy"), as.factor)
test_set <- test_set %>% mutate_at(c("economy"), as.factor)
train_set <- train_set[,-21]
```


```{r}
arbol_modelo <- rpart(economy~., train_set, method="class")
rpart.plot(arbol_modelo)
```


## 8 - Utilizando modelo con el conjunto de prueba para determinar eficiencia del algoritmo

```{r}
y8 <- test_set[,c("salePrice")]

y_pred_8 <- predict(arbol_modelo, newdata = test_set)
rmse_model_8 <- RMSE(pred = y_pred_8, obs = test_set$salePrice)
abs_error_8 <- abs(mean(y_pred_8 - test_set$salePrice))
mean_error_8 <- mean(y8-y_pred_8)
```
El RMSE para el modelo de árbol de clasificación es `r rmse_model_8` y el error absoluto es `r abs_error_8`. Tomando en cuenta los valores reales y los que se predijeron se sabe que en promedio hubo un error en promedio de USD `r mean_error_8`.


## 9 - Análisis de eficiencia usando matriz de confusión

```{r}
y9 <- test_set[,24]
test_set <- test_set[,-24]

ypred9 <- predict(arbol_modelo, newdata = test_set)
ypred9 <- apply(ypred9, 1, function(x) colnames(ypred9)[which.max(x)])
ypred9 <- factor(ypred9)

confusionMatrix(ypred9, y9)
```
Nos da un 80.72% de accuracy con esto se puede deducir que el modelo no está mal, sin embargo no es posible concluir con que indica que el modelo sea tan bueno. Además, se puede ver que predijo 60 valores para categoria average correctamente, 165 valores para la categoria economic correctamente y 68 valores para la categoria expensive correctamente. Donde más fallos en la predicción hubo fue cuando predijo que 26 valores pertenecian a la categoria economic cuando verdaderamente pertenecian a la categoria average.

## 10 - Entrenación de modelo con cross validation

```{r}
ct <- trainControl(method="cv", number=10, verboseIter=T)
modelo10 <- train(train_set[-22], train_set$economy, trControl = ct, method = "rpart")
```


## 11 - Crear 3 modelos más cambiando la profundidad del árbol

Modelo extra #1
``` {r}
model_extra_1 <- rpart(
  formula = salePrice ~ neighborhood + houseStyle + houseZone + houseUtilities + remodelated + pool + livingArea + yearBuilt
                        + overallQuality + overallCondition,
  data    = train_set,
  method  = "anova",
  control = list(maxdepth = 0)
  )
rpart.plot(model_extra_1)
```

Modelo extra #2
``` {r}
model_extra_2 <- rpart(
  formula = salePrice ~ neighborhood + houseStyle + houseZone + houseUtilities + remodelated + pool + livingArea + yearBuilt
                        + overallQuality + overallCondition,
  data    = train_set,
  method  = "anova",
  control = list(maxdepth = 7)
  )
rpart.plot(model_extra_2)
```

Modelo extra #3
``` {r}
model_extra_3 <- rpart(
  formula = salePrice ~ neighborhood + houseStyle + houseZone + houseUtilities + remodelated + pool + livingArea + yearBuilt
                        + overallQuality + overallCondition,
  data    = train_set,
  method  = "anova",
  control = list(maxdepth = 14)
  )
rpart.plot(model_extra_3)
```

Vamos a determinar cual es el modelo que funciono mejor por medio de el coeficiente de determinación (R^2)

```{r}
y_pred_extra_1 <- predict(model_extra_1, newdata = test_set)
abs_error_extra_1 <- abs(mean(y_pred_extra_1 - test_set$salePrice))

y_pred_extra_2 <- predict(model_extra_2, newdata = test_set)
abs_error_extra_2 <- abs(mean(y_pred_extra_2 - test_set$salePrice))

y_pred_extra_3 <- predict(model_extra_3, newdata = test_set)
abs_error_extra_3 <- abs(mean(y_pred_extra_3 - test_set$salePrice))
```

Se mirara cual de las MAE tiene el valor mas bajo

```{r}
model_results_extra <- data.frame(
  c("Modelo Extra 1", "Modelo Extra 2", "Modelo Extra 3"),
  c(0, 7, 14),
  c(abs_error_extra_1, abs_error_extra_2, abs_error_extra_3)
)

colnames(model_results_extra) <- c("", "Profundidad", "MEA")
kable(model_results_extra)
```

Como se puede ver en la tabla se llevo a cabo arboles con profundidad 0, 7 y 14. 
La profundidad 0 fue el que se desempeño peor (Y tiene sentido ya que no hace lo esperado), mientras que la profundidad 7 y 14 se desempeñaron igual y son los que se desempeñaron mejor (hablando exclusivamente de los modelos extras)

Entre todos los modelos el que mejor funciona es el modelo cuyo valor de profundidad es de 3

### 12 - Repetir los análisis usando random forest como algoritmos de predicción 

```{r}

```

